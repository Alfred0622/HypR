# HypR

## Abstract
With the development of deep learning, automatic speech recognition (ASR) has made significant progress. To further enhance the performance, revising recognition results is one of the lightweight but efficient manners. Various methods can be roughly classified into N-best reranking methods and error correction models. The former aims to select the hypothesis with the lowest error rate from a set of candidates generated by ASR for a given input speech. The latter focuses on detecting recognition errors in a given hypothesis and correcting these errors to obtain an enhanced result. However, we observe that these studies are hardly comparable to each other as they are usually evaluated on different corpora, paired with different ASR models, and even use different datasets to train the models. Accordingly, we first concentrate on releasing an ASR hypothesis revising (HypR) dataset in this study. HypR contains several commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provides 50 recognition hypotheses for each speech utterance. The checkpoint models of the ASR are also published. In addition, we implement and compare several classic and representative methods, showing the recent research progress in revising speech recognition results. We hope the publicly available HypR dataset can become a reference benchmark for subsequent research and promote the school of research to an advanced level.

> The dataset will be available soon

## Licence
The HypR dataset is derived from AISHELL-1, AISHELL-2, TED-LIUM 2 and LibriSpeech corpus.

AISHELL-1 and AISHELL-2 is supported by AISHELL fundation,  TED-LIUM 2 is an English speech corpus supported by LIUM lab and LibriSpeech is an English audio book dataset prepared by Vassil et al.

HypR follows the license of these 4 datasets. It is free to use for academic purpose and **SHOULD NOT** be used on any commercial purpose witout the permission  from these fundations.
```
@INPROCEEDINGS{AISHELL1,
  author={Bu, Hui and Du, Jiayu and Na, Xingyu and Wu, Bengu and Zheng, Hao},
  booktitle={Proceedings of O-COCOSDA}, 
  title={AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline}, 
  year={2017},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICSDA.2017.8384449}}

```

```
@article{AISHELL2,
  author       = {Jiayu Du and
                  Xingyu Na and
                  Xuechen Liu and
                  Hui Bu},
  title        = {{AISHELL-2:} Transforming Mandarin {ASR} Research Into Industrial
                  Scale},
  journal      = {CoRR},
  volume       = {abs/1808.10583},
  year         = {2018},
  url          = {http://arxiv.org/abs/1808.10583},
  eprinttype    = {arXiv},
  eprint       = {1808.10583},
  timestamp    = {Mon, 03 Sep 2018 13:36:40 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1808-10583.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
```

```
@inproceedings{TEDLIUM2,
    title = "Enhancing the {TED}-{LIUM} Corpus with Selected Data for Language Modeling and More {TED} Talks",
    author = "Rousseau, Anthony  and
      Del{\'e}glise, Paul  and
      Est{\`e}ve, Yannick",
    booktitle = "Proceedings of LREC'14",
    year = "2014",
    pages = "3935--3939",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/1104_Paper.pdf",
    abstract = "In this paper, we present improvements made to the TED-LIUM corpus we released in 2012. These enhancements fall into two categories. First, we describe how we filtered publicly available monolingual data and used it to estimate well-suited language models (LMs), using open-source tools. Then, we describe the process of selection we applied to new acoustic data from TED talks, providing additions to our previously released corpus. Finally, we report some experiments we made around these improvements.",
}
```

```
@INPROCEEDINGS{LibriSpeech,
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle= {Proceedings of ICASSP}, 
  title={Librispeech: An ASR corpus based on public domain audio books}, 
  year={2015},
  volume={},
  number={},
  pages={5206-5210},
  doi={10.1109/ICASSP.2015.7178964}}
```
---
If you find our work useful, please cite
```
@misc{wang2023hypr,
      title={HypR: A comprehensive study for ASR hypothesis revising with a reference corpus}, 
      author={Yi-Wei Wang and Ke-Han Lu and Kuan-Yu Chen},
      year={2023},
      eprint={2309.09838},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
